{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5b777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Content-Type: text/html;\\nContent-Transfer-Enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi, i've just updated from the gulus and I che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Content-Type: text/plain;\\n\\tcharset=\"iso-8859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey Billy, \\n\\nit was really fun going out the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Content-Type: multipart/alternative;\\n        ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      1  Content-Type: text/html;\\nContent-Transfer-Enc...\n",
       "1      0  Hi, i've just updated from the gulus and I che...\n",
       "2      1  Content-Type: text/plain;\\n\\tcharset=\"iso-8859...\n",
       "3      1  Hey Billy, \\n\\nit was really fun going out the...\n",
       "4      1  Content-Type: multipart/alternative;\\n        ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec = pd.read_csv('TREC2007.csv').drop(columns=['subject', 'email_to', 'email_from']).dropna()\n",
    "\n",
    "spam_ham = pd.read_csv('spam_ham_dataset.csv').drop(columns=['label', 'Unnamed: 0']).dropna()\n",
    "spam_ham = spam_ham.rename(columns={\"text\": \"message\", \"label_num\": \"label\"})\n",
    "\n",
    "spam = pd.read_csv('spam.csv', encoding='latin-1').drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']).dropna()\n",
    "spam = spam.rename(columns={\"v1\": \"label\", \"v2\": \"message\"})\n",
    "spam['label'] = spam['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "sms_spam = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message']).dropna()\n",
    "sms_spam['label'] = sms_spam['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "enron = pd.read_csv('enron.csv').dropna()\n",
    "enron['label'] = enron['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "gpt = pd.read_csv('TREC2007.csv').drop(columns=['subject', 'email_to', 'email_from']).dropna()\n",
    "\n",
    "\n",
    "data = pd.concat([trec, spam_ham, spam, sms_spam, enron], ignore_index=True)\n",
    "gptdata = pd.concat([data, gpt])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.1, random_state=42)\n",
    "label = data['label']\n",
    "# gptdata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6260e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "TF_vectorizer = count_vectorizer.fit_transform(data['message'])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "TFIDF_vectorizer = tfidf_vectorizer.fit_transform(data['message'])\n",
    "\n",
    "NB_TF_class = MultinomialNB()\n",
    "NB_TF_class.fit(TF_vectorizer, label)\n",
    "\n",
    "NB_TFIDF_class = MultinomialNB()\n",
    "NB_TFIDF_class.fit(TFIDF_vectorizer, label)\n",
    "\n",
    "reg_TF_classifier = LogisticRegression(solver='sag')\n",
    "reg_TF_classifier.fit(TF_vectorizer, label)\n",
    "\n",
    "reg_TFIDF_classifier = LogisticRegression(solver='sag')\n",
    "reg_TFIDF_classifier.fit(TFIDF_vectorizer, label)\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(random_state=40, n_repeats=3, n_splits=5)\n",
    "NB_TF_scores = cross_val_score(NB_TF_class, TF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')\n",
    "NB_TFIDF_scores = cross_val_score(NB_TFIDF_class, TFIDF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')\n",
    "reg_TF_scores = cross_val_score(reg_TF_classifier, TF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')\n",
    "reg_TFIDF_scores = cross_val_score(reg_TFIDF_classifier, TFIDF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\a393737\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "label = gptdata['label']\n",
    "\n",
    "gptcount_vectorizer = CountVectorizer()\n",
    "gptTF_vectorizer = gptcount_vectorizer.fit_transform(gptdata['message'])\n",
    "\n",
    "gpttfidf_vectorizer = TfidfVectorizer()\n",
    "gptTFIDF_vectorizer = gpttfidf_vectorizer.fit_transform(gptdata['message'])\n",
    "\n",
    "gptNB_TF_class = MultinomialNB()\n",
    "gptNB_TF_class.fit(gptTF_vectorizer, label)\n",
    "\n",
    "gptNB_TFIDF_class = MultinomialNB()\n",
    "gptNB_TFIDF_class.fit(gptTFIDF_vectorizer, label)\n",
    "\n",
    "gptreg_TF_classifier = LogisticRegression(solver='sag')\n",
    "gptreg_TF_classifier.fit(gptTF_vectorizer, label)\n",
    "\n",
    "gptreg_TFIDF_classifier = LogisticRegression(solver='sag')\n",
    "gptreg_TFIDF_classifier.fit(gptTFIDF_vectorizer, label)\n",
    "\n",
    "# rskf = RepeatedStratifiedKFold(random_state=40, n_repeats=3, n_splits=5)\n",
    "gptNB_TF_scores = cross_val_score(gptNB_TF_class, gptTF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')\n",
    "gptNB_TFIDF_scores = cross_val_score(gptNB_TFIDF_class, gptTFIDF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')\n",
    "gptreg_TF_scores = cross_val_score(gptreg_TF_classifier, gptTF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')\n",
    "gptreg_TFIDF_scores = cross_val_score(gptreg_TFIDF_classifier, gptTFIDF_vectorizer, label, cv=rskf, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2789439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert scores to a pandas DataFrame\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\__init__.py:977\u001b[0m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[0;32m    976\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[1;32m--> 977\u001b[0m     \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[0;32m    980\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\cbook.py:545\u001b[0m, in \u001b[0;36m_get_data_path\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data_path\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_path\u001b[49m(), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert scores to a pandas DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'TF naive Bayes': NB_TF_scores,\n",
    "    'gptTF naive Bayes': gptNB_TF_scores,\n",
    "    'TF-IDF naive Bayes': NB_TFIDF_scores,\n",
    "    'gptTF-IDF naive Bayes': gptNB_TFIDF_scores,\n",
    "    'TF logistic regression': reg_TF_scores,\n",
    "    'gptTF logistic regression': gptreg_TF_scores,\n",
    "    'TF-IDF logistic regression': reg_TFIDF_scores,\n",
    "    'gptTF-IDF logistic regression': gptreg_TFIDF_scores\n",
    "})\n",
    "\n",
    "# Summary table\n",
    "mean_scores = results.mean()\n",
    "std_scores = results.std()\n",
    "summary_table = pd.DataFrame({'Mean Accuracy': mean_scores, 'Standard Deviation': std_scores})\n",
    "print(\"Summary of Results:\")\n",
    "print(summary_table)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=results)\n",
    "# plt.title('Comparison of Model Accuracies (without chat GPT dataset)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43671e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-25.51564823761338, pvalue=6.260521152971419e-21, df=28.0)\n",
      "TtestResult(statistic=0.8145790408855447, pvalue=0.42218773803594567, df=28.0)\n",
      "TtestResult(statistic=-0.8068359678765069, pvalue=0.4265616299959213, df=28.0)\n",
      "TtestResult(statistic=-11.957609871085092, pvalue=1.6193671032393531e-12, df=28.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.ttest_ind(NB_TF_scores, gptNB_TF_scores))\n",
    "print(stats.ttest_ind(NB_TFIDF_scores, gptNB_TFIDF_scores))\n",
    "print(stats.ttest_ind(reg_TF_scores, gptreg_TF_scores))\n",
    "print(stats.ttest_ind(reg_TFIDF_scores, gptreg_TFIDF_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e93df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'enron/enron1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      6\u001b[0m temp_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menron/enron1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      8\u001b[0m         folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(temp_dir, folder_name)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(folder_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'enron/enron1'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "temp_dir = 'enron/enron1'\n",
    "for folder_name in os.listdir(temp_dir):\n",
    "        folder_path = os.path.join(temp_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming each file is a CSV with a header\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                        file_content = file.read()\n",
    "                    temp_df = pd.DataFrame({'message': [file_content], 'label': [folder_name]})\n",
    "                    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "temp_dir = 'enron/enron2'\n",
    "for folder_name in os.listdir(temp_dir):\n",
    "        folder_path = os.path.join(temp_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming each file is a CSV with a header\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                        file_content = file.read()\n",
    "                    temp_df = pd.DataFrame({'message': [file_content], 'label': [folder_name]})\n",
    "                    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "temp_dir = 'enron/enron3'\n",
    "for folder_name in os.listdir(temp_dir):\n",
    "        folder_path = os.path.join(temp_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming each file is a CSV with a header\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                        file_content = file.read()\n",
    "                    temp_df = pd.DataFrame({'message': [file_content], 'label': [folder_name]})\n",
    "                    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "temp_dir = 'enron/enron4'\n",
    "for folder_name in os.listdir(temp_dir):\n",
    "        folder_path = os.path.join(temp_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming each file is a CSV with a header\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                        file_content = file.read()\n",
    "                    temp_df = pd.DataFrame({'message': [file_content], 'label': [folder_name]})\n",
    "                    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "temp_dir = 'enron/enron5'\n",
    "for folder_name in os.listdir(temp_dir):\n",
    "        folder_path = os.path.join(temp_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming each file is a CSV with a header\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                        file_content = file.read()\n",
    "                    temp_df = pd.DataFrame({'message': [file_content], 'label': [folder_name]})\n",
    "                    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "temp_dir = 'enron/enron6'\n",
    "for folder_name in os.listdir(temp_dir):\n",
    "        folder_path = os.path.join(temp_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming each file is a CSV with a header\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                        file_content = file.read()\n",
    "                    temp_df = pd.DataFrame({'message': [file_content], 'label': [folder_name]})\n",
    "                    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('enron.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF\n",
      "Accuracy: 0.91 (+/- 0.00)\n",
      "Number of CV Scores used in Average:  15\n",
      "\n",
      "TFIDF\n",
      "Accuracy: 0.96 (+/- 0.00)\n",
      "Number of CV Scores used in Average:  15\n",
      "\n",
      "Regresja\n",
      "Accuracy: 0.98 (+/- 0.00)\n",
      "Number of CV Scores used in Average:  15\n"
     ]
    }
   ],
   "source": [
    "print(\"TF\")\n",
    "# print(\"Cross Validation Scores: \\n\", tfscores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (NB_TF_scores.mean(), NB_TF_scores.std() * 2))\n",
    "print(\"Number of CV Scores used in Average: \", len(NB_TF_scores))\n",
    "\n",
    "print(\"\\nTFIDF\")\n",
    "# print(\"\\nCross Validation Scores: \\n\", tfidfscores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (NB_TFIDF_scores.mean(), NB_TFIDF_scores.std() * 2))\n",
    "print(\"Number of CV Scores used in Average: \", len(NB_TFIDF_scores))\n",
    "\n",
    "print(\"\\nRegresja\")\n",
    "# print(\"Cross Validation Scores: \\n\", tfscores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (reg_TFIDF_scores.mean(), reg_TFIDF_scores.std() * 2))\n",
    "print(\"Number of CV Scores used in Average: \", len(reg_TFIDF_scores))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
